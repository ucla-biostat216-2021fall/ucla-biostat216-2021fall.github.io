{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 216 Final\n",
    "\n",
    "Dec 9, 2021, 3pm-6pm, CHS 41-268\n",
    "\n",
    "In class, closed-book, one page (letter size, double-sided) cheat sheet allowed.\n",
    "\n",
    "Make sure to write your **name** and **UID** on your answer sheets. Also number the answer sheets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q1. (9pts). Let \n",
    "$$\n",
    "\\mathbf{S} = \\begin{pmatrix} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & 5 \\end{pmatrix} \\begin{pmatrix} \\cos \\theta & \\sin \\theta \\\\ -\\sin \\theta & \\cos \\theta \\end{pmatrix}.\n",
    "$$\n",
    "Find (a) the determinant of $\\mathbf{S}$, (b) the trace of $\\mathbf{S}$ (sum of diagonal entries), (c) the eigenvalues of $\\mathbf{S}$, (d) the eigenvectors of $\\mathbf{S}$, (e) the eigenvalues of $\\mathbf{S}^5$, (f) the eigenvalues of $\\mathbf{S} - 0.5 \\mathbf{I}_2$, (g) a reason why $\\mathbf{S}$ is positive definite, (h) the singular value decomposition (SVD) of $\\mathbf{S}$, (i) a best rank-1 approximation (in terms of Frobenius or spectral norm) to $\\mathbf{S}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q2. (6pts) \n",
    "    1. Show that the eigenvectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$, corresponding to distinct eigenvalues $\\lambda_1$ and $\\lambda_2$, of a square matrix are linearly independent. Hint: proof by contradiction.  \n",
    "    2. Show that the eigenvectors $\\mathbf{x}_1$ and $\\mathbf{x}_2$, corresponding to distinct eigenvalues $\\lambda_1$ and $\\lambda_2$, of a _symmetric_ matrix are orthogonal to each other. Hint: show $\\mathbf{x}_2 \\in \\mathcal{N}(\\mathbf{A}-\\lambda_2 \\mathbf{I})$ and $\\mathbf{x}_1 \\in \\mathcal{C}(\\mathbf{A}-\\lambda_2 \\mathbf{I})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q3. (5pts) (Closest point theorem) Let $\\mathcal{S}$ be a vector space in $\\mathbb{R}^n$. Show that if $\\mathbf{u}$ is the orthogonal projection of $\\mathbf{y} \\in \\mathbb{R}^n$ into $\\mathcal{S}$, then\n",
    "$$\n",
    "\\|\\mathbf{y} - \\mathbf{u}\\|^2 \\le \\|\\mathbf{y} - \\mathbf{w}\\|^2\n",
    "$$\n",
    "for all $\\mathbf{w} \\in \\mathcal{S}$. In words, $\\mathbf{u}$ is the closest point in $\\mathcal{S}$ to $\\mathbf{y}$. (In statistics, this result tells us least squares solution gives us the best fit to a data vector $\\mathbf{y}$ using predictors in $\\mathbf{X}$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q4. (6pts) $\\mathbf{A} \\in \\mathbb{R}^{15 \\times 10}$ has rank $5$.   \n",
    "    1. Give the dimensions of the $\\mathbf{U}$, $\\boldsymbol{\\Sigma}$, and $\\mathbf{V}$ matrices in the singular value decomposition (SVD) of $\\mathbf{A}$.  \n",
    "    \n",
    "    2. Give the dimensions of the $\\mathbf{U}$, $\\boldsymbol{\\Sigma}$, and $\\mathbf{V}$ matrices in the **reduced-form** (or **thin**) SVD of $\\mathbf{A}$.\n",
    "    \n",
    "    3. Give the dimensions of the $\\mathbf{U}$, $\\boldsymbol{\\Sigma}$, and $\\mathbf{V}$ matrices in the **full** SVD of $\\mathbf{A}$.\n",
    "    \n",
    "    4. Give the dimensions of the $\\mathbf{Q}$ and $\\boldsymbol{\\Lambda}$ matrices in the spectral decomposition of the Gram matrix $\\mathbf{A}'\\mathbf{A} = \\mathbf{Q} \\boldsymbol{\\Lambda} \\mathbf{Q}'$.\n",
    "    \n",
    "    5. Give the dimensions of the $\\mathbf{Q}$ and $\\boldsymbol{\\Lambda}$ matrices in the spectral decomposition of the Gram matrix $\\mathbf{A}\\mathbf{A}' = \\mathbf{Q} \\boldsymbol{\\Lambda} \\mathbf{Q}'$.  \n",
    "    \n",
    "    6. Give the dimension of the Moore-Penrose generalized inverse $\\mathbf{A}^+$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q5. (5pts) Find the orthogonal projection of the point $\\mathbf{1}_3$ into the plane spanned by the vectors $\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} -2 \\\\ 2 \\\\ 1 \\end{pmatrix}$. Hint: You can either use $\\mathbf{P} = \\mathbf{X} (\\mathbf{X}'\\mathbf{X})^{-1} \\mathbf{X}'$ or $\\mathbf{P} = \\mathbf{Q} \\mathbf{Q}'$ where columns of $\\mathbf{Q}$ are an orthonormal basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q6. (5pts) Let \n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 6 & 1 & 0 \\\\ -8 & 5 & 3 \\end{pmatrix} \\begin{pmatrix} 2 & 6 & -8 \\\\ 0 & 1 & 5 \\\\ 0 & 0 & 3 \\end{pmatrix}.\n",
    "$$\n",
    "    1. Is $\\mathbf{A}$ a positive definite matrix? Why?  \n",
    "    2. Calculate $\\det (\\mathbf{A})$, $\\det (\\mathbf{A}^3)$, $\\det (\\mathbf{A}^{-1})$, and $\\det (-2\\mathbf{A})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q7. (9pts) (Rayleigh quotient) Suppose $\\mathbf{S} \\in \\mathbb{R}^{n \\times n}$ is positive definite with eigenvalues $\\lambda_1 > \\lambda_2 \\ge \\cdots \\ge \\lambda_n > 0$ and corresponding eigenvectors $\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n$. \n",
    "    1. Show that the maximum value of the Rayleigh quotient\n",
    "    $$\n",
    "    R(\\mathbf{x}) = \\frac{\\mathbf{x}' \\mathbf{S} \\mathbf{x}}{\\mathbf{x}' \\mathbf{x}}\n",
    "    $$\n",
    "    is $\\lambda_1$.  \n",
    "    2. Show that the maximum value of the Rayleigh quotient $R(\\mathbf{x})$, subjec to the constraint $\\mathbf{x} \\perp \\mathbf{u}_1$, is $\\lambda_2$.  \n",
    "    3. Show that the minimum value of the Rayleigh quotient $R(\\mathbf{x})$ is $\\lambda_n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q8. (5pts) Fundamental theorem of ranks.\n",
    "    1. State the rank-nullity theorem. You don't need to prove it.  \n",
    "    2. Show that $\\mathcal{N}(\\mathbf{A}'\\mathbf{A}) = \\mathcal{N}(\\mathbf{A})$.  \n",
    "    3. Show the fundamental theorem of ranks: $\\text{rank}(\\mathbf{A}) = \\text{rank}(\\mathbf{A}'\\mathbf{A})$. Hint: use 1 and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q9. (5pts) Let $\\mathbf{A}$ and $\\mathbf{B}$ be two positive semidefinite matrices of same size. Show that $\\alpha \\mathbf{A} + \\beta \\mathbf{B}$ is positive semidefinite for any $\\alpha, \\beta \\ge 0$. Give a counter-example to show that it is not true if $\\alpha$ and $\\beta$ are allowed to be negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q10. (5pts) Let $\\mathbf{A} = \\mathbf{x} \\mathbf{y}'$, where $\\mathbf{x} \\in \\mathbb{R}^m$ and $\\mathbf{y} \\in \\mathbb{R}^n$ are non-zero vectors. \n",
    "    1. What is the rank of $\\mathbf{A}$? \n",
    "    2. Find the reduced-form SVD of $\\mathbf{A}$. \n",
    "    3. Find the Moore-Penrose inverse of $\\mathbf{A}$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
