{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Orthogonal-projections\" data-toc-modified-id=\"Orthogonal-projections-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Orthogonal projections</a></div><div class=\"lev2 toc-item\"><a href=\"#Row-rank-=-column-rank:-a-proof-using-orthogonality-(optional)\" data-toc-modified-id=\"Row-rank-=-column-rank:-a-proof-using-orthogonality-(optional)-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Row rank = column rank: a proof using orthogonality (optional)</a></div><div class=\"lev2 toc-item\"><a href=\"#Ortho-complementary-subspaces\" data-toc-modified-id=\"Ortho-complementary-subspaces-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Ortho-complementary subspaces</a></div><div class=\"lev2 toc-item\"><a href=\"#The-fundamental-theorem-of-linear-algebra\" data-toc-modified-id=\"The-fundamental-theorem-of-linear-algebra-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>The fundamental theorem of linear algebra</a></div><div class=\"lev2 toc-item\"><a href=\"#Orthogonal-matrix\" data-toc-modified-id=\"Orthogonal-matrix-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Orthogonal matrix</a></div><div class=\"lev2 toc-item\"><a href=\"#Orthogonal-projections\" data-toc-modified-id=\"Orthogonal-projections-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Orthogonal projections</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orthogonal projections\n",
    "\n",
    "**Highlights** In this lecture, we'll show (1) the **fundamental theorem of linear algebra**: \n",
    "\n",
    "<img src=\"../04-vecsp/four_fundamental_subspaces.png\" width=400 align=\"center\"/>\n",
    "\n",
    "and (2) any symmetric, idempotent matrix $\\mathbf{P}$ is the orthogonal projector onto $\\mathcal{C}(\\mathbf{P})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row rank = column rank: a proof using orthogonality (optional)\n",
    "    \n",
    "We show that the row rank of a matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ is equal to its column rank.\n",
    "\n",
    "Let $r$ be the row rank of $\\mathbf{A}$ and $\\mathbf{x}_1, \\ldots, \\mathbf{x}_r \\in \\mathbb{R}^n$ be a basis of the row space of $\\mathbf{A}$. \n",
    "\n",
    "- Lemma: The vectors $\\mathbf{A} \\mathbf{x}_1, \\ldots, \\mathbf{A} \\mathbf{x}_r$ are linearly independent.\n",
    "\n",
    "    Proof: Suppose\n",
    "$$\n",
    "    \\sum_{i=1}^r c_i \\mathbf{A} \\mathbf{x}_i = \\mathbf{A} \\sum_{i=1}^r c_i \\mathbf{x}_i = \\mathbf{A} \\mathbf{v} = \\mathbf{0},\n",
    "$$\n",
    "where $\\mathbf{v} = \\sum_{i=1}^r c_i \\mathbf{x}_i$. $\\mathbf{v}$ is a linear combination of vectors in $\\mathcal{R}(\\mathbf{A})$ so $\\mathbf{v} \\in \\mathcal{R}(\\mathbf{A})$. Since $\\mathbf{A} \\mathbf{v} = \\mathbf{0}$, $\\mathbf{v}$ is orthogonal to each row of $\\mathbf{A}$ thus is orthogonal to all vectors in $\\mathcal{R}(\\mathbf{A})$, including itself. Thus $\\mathbf{v}=\\mathbf{0}$ and $c_i$ are zero for all $i$. Thus we have shown $\\mathbf{A} \\mathbf{x}_1, \\ldots, \\mathbf{A} \\mathbf{x}_r$ are linearly independent.\n",
    "\n",
    "By the lemma, the column rank of $\\mathbf{A}$ must be $\\ge r$. Now apply the same argument to $\\mathbf{A}'$, we have the column rank of $\\mathbf{A}$ is $\\le r$. \n",
    "\n",
    "The lemma also provides a convenient way to produce a basis for the column space given a basis for the row space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ortho-complementary subspaces\n",
    "\n",
    "- An **orthocomplement set** of a set $\\mathcal{X}$ (not necessarily a subspace) in a vector space $\\mathcal{V} \\subseteq \\mathbb{R}^m$ is defined as\n",
    "$$\n",
    "    \\mathcal{X}^\\perp = \\{ \\mathbf{u} \\in \\mathcal{V}: \\langle \\mathbf{x}, \\mathbf{u} \\rangle = 0 \\text{ for all } \\mathbf{x} \\in \\mathcal{X}\\}.\n",
    "$$\n",
    "\n",
    "- TODD: visualize $\\mathbb{R}^3 = \\text{a plane} \\oplus \\text{plan}^\\perp$.\n",
    "    \n",
    "- **Direct sum theorem for orthocomplementary subspaces.** Let $\\mathcal{S}$ be a subspace of a vector space $\\mathcal{V}$ with $\\text{dim}(\\mathcal{V}) = m$. Then the following statements are true.    \n",
    "    1. $\\mathcal{V} = \\mathcal{S} + \\mathcal{S}^\\perp$. Or every vector $\\mathbf{y} \\in \\mathcal{V}$ can be expressed as $\\mathbf{y} = \\mathbf{u} + \\mathbf{v}$, where $\\mathbf{u} \\in \\mathcal{S}$ and $\\mathbf{v} \\in \\mathcal{S}^\\perp$.  \n",
    "    2. $\\mathcal{S} \\cap \\mathcal{S}^\\perp = \\{\\mathbf{0}\\}$ (essentially disjoint).  \n",
    "    3. $\\mathcal{V} = \\mathcal{S} \\oplus \\mathcal{S}^\\perp$.  \n",
    "    4. $m = \\text{dim}(\\mathcal{S}) + \\text{dim}(\\mathcal{S}^\\perp)$.  \n",
    "    By the uniqueness of decomposition for direct sum, we know the expression of $\\mathbf{y} = \\mathbf{u} + \\mathbf{v}$ is also **unique**.\n",
    "\n",
    "    Proof of 1 (TODO): Let $\\mathbf{z}_1, \\ldots, \\mathbf{z}_r$ be an orthonormal basis of $\\mathcal{S}$, e.g., by applying Gram-Schmidt to a basis of $\\mathcal{S}$. And let $\\mathbf{z}_{r+1}, \\ldots, \\mathbf{z}_m$ be an orthonormal basis of $\\mathcal{S}^\\perp$. Then\n",
    "$$\n",
    "    \\mathbf{y} = \\sum_{i=1}^r \\langle \\mathbf{y}, \\mathbf{z}_i \\rangle \\mathbf{z}_i + \\sum_{i=r+1}^m \\langle \\mathbf{y}, \\mathbf{z}_i \\rangle \\mathbf{z}_i,\n",
    "$$\n",
    "where the first sum belongs to $\\mathcal{S}$ and the second to $\\mathcal{S}^\\perp$.  \n",
    "    Proof of 2: Suppose $\\mathbf{x} \\in \\mathcal{S} \\cap \\mathcal{S}^\\perp$, then $\\mathbf{x} \\perp \\mathbf{x}$, i.e., $\\langle \\mathbf{x}, \\mathbf{x} \\rangle = 0$. Therefore $\\mathbf{x} = \\mathbf{0}$.  \n",
    "    Proof of 3: Statement 1 says $\\mathcal{V} = \\mathcal{S} + \\mathcal{S}^\\perp$. Statement 2 says $\\mathcal{S}$ and $\\mathcal{S}^\\perp$ are essentially disjoint. Thus $\\mathcal{V} = \\mathcal{S} \\oplus \\mathcal{S}^\\perp$.  \n",
    "    Proof of 4: Follows from essential disjointness between $\\mathcal{S}$ and $\\mathcal{S}^\\perp$.  \n",
    "    \n",
    "- Some facts:  \n",
    "    1. For a set $\\mathcal{X}$ in a vector space $\\mathcal{V}$, $\\mathcal{X}^\\perp$ is always a subspace, whether or not $\\mathcal{X}$ is a subspace.  \n",
    "    2. If $\\mathcal{S}$ is a subspace of $\\mathcal{V}$, then $(\\mathcal{S}^\\perp)^\\perp = \\mathcal{S}$.  \n",
    "    3. If $\\mathcal{S}_1 \\subseteq \\mathcal{S}_2$ are subsets of $\\mathcal{V}$, then $\\mathcal{S}_1^\\perp \\supseteq \\mathcal{S}_2^\\perp$.  \n",
    "    4. If $\\mathcal{S}_1 = \\mathcal{S}_2$ are two subsets of $\\mathcal{V}$, then $\\mathcal{S}_1^\\perp = \\mathcal{S}_2^\\perp$.  \n",
    "    5. If $\\mathcal{S}_1$ and $\\mathcal{S}_2$ are two subspaces in $\\mathcal{V}$, then $(\\mathcal{S}_1 + \\mathcal{S}_2)^\\perp = \\mathcal{S}_1^\\perp \\cap \\mathcal{S}_2^\\perp$ and $(\\mathcal{S}_1 \\cap \\mathcal{S}_2)^\\perp = \\mathcal{S}_1^\\perp + \\mathcal{S}_2^\\perp$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fundamental theorem of linear algebra\n",
    "\n",
    "- Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. Then\n",
    "    1. $\\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}')$ and $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$.  \n",
    "    2. $\\mathcal{C}(\\mathbf{A}) = \\mathcal{N}(\\mathbf{A}')^\\perp$.  \n",
    "    3. $\\mathcal{N}(\\mathbf{A})^\\perp = \\mathcal{C}(\\mathbf{A}')$ and $\\mathbb{R}^n = \\mathcal{N}(\\mathbf{A}) \\oplus \\mathcal{C}(\\mathbf{A}')$.  \n",
    "    \n",
    "    Proof of 1: To show $\\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}')$,\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{x} \\in \\mathcal{N}(\\mathbf{A}') \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{A}' \\mathbf{x} = \\mathbf{0} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{x} \\text{ is orthogonal to columns of } \\mathbf{A} \\\\\n",
    "    &\\Leftrightarrow& \\mathbf{x} \\in \\mathcal{C}(\\mathbf{A})^\\perp.\n",
    "\\end{eqnarray*}\n",
    "Then, $\\mathbb{R}^m = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{C}(\\mathbf{A})^\\perp = \\mathcal{C}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A}')$.  \n",
    "\n",
    "    Proof of 2: Since $\\mathcal{C}(\\mathbf{A})$ is a subspace, $(\\mathcal{C}(\\mathbf{A})^\\perp)^\\perp = \\mathcal{N}(\\mathbf{A}')^\\perp$. \n",
    "    \n",
    "    Proof of 3: Applying part 2 to $\\mathbf{A}'$, we have\n",
    "$$\n",
    "    \\mathcal{C}(\\mathbf{A}') = \\mathcal{N}((\\mathbf{A}')')^\\perp = \\mathcal{N}(\\mathbf{A})^\\perp\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    \\mathbb{R}^n = \\mathcal{N}(\\mathbf{A}) \\oplus \\mathcal{N}(\\mathbf{A})^\\perp = \\mathcal{N}(\\mathbf{A}) \\oplus \\mathcal{C}(\\mathbf{A}').\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal matrix\n",
    "\n",
    "- Let $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$. The following statements are equivalent:  \n",
    "    1. The columns of $\\mathbf{Q}$ are orthonormal vectors.  \n",
    "    2. $\\mathbf{Q}'\\mathbf{Q} = \\mathbf{I} = \\mathbf{Q} \\mathbf{Q}'$.  \n",
    "    3. The rows of $\\mathbf{Q}$ are orthonormal vectors.  \n",
    "    \n",
    "    Proof: We show $1 \\Rightarrow 2 \\Rightarrow 3 \\Rightarrow 1$.  \n",
    "    \n",
    "    Proof of $1 \\Rightarrow 2$: Let $\\mathbf{Q} = (\\mathbf{q}_1 \\, \\cdots \\, \\mathbf{q}_n)$. Orthonormality of $\\mathbf{q}_i$ shows $\\mathbf{Q}' \\mathbf{Q} = \\mathbf{I}$. Also $\\mathbf{q}_i$ are linearly independent thus $\\mathbf{Q}$ is non-singular. Thus $\\mathbf{Q}^{-1} = \\mathbf{Q}'$ and $\\mathbf{Q} \\mathbf{Q}' = \\mathbf{Q} \\mathbf{Q}^{-1} = \\mathbf{I}$.  \n",
    "    \n",
    "    Proof of $2 \\Rightarrow 3$: $\\mathbf{Q} \\mathbf{Q}' = \\mathbf{I}$ shows that the rows of $\\mathbf{Q}$ are orthonormal.\n",
    "    \n",
    "    Proof of $3 \\Rightarrow 1$: Statement 3 says columns of $\\mathbf{Q}'$ are orthonormal. We have shown $1 \\Rightarrow 2 \\Rightarrow 3$. Thus we know rows of $\\mathbf{Q}'$ are orthonormal, i.e., columns of $\\mathbf{Q}$ are orthonormal.  \n",
    "\n",
    "- Any square matrix $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ satisfying $\\mathbf{Q} \\mathbf{Q}' = \\mathbf{Q}' \\mathbf{Q} = \\mathbf{I}$ is called an **orthogonal matrix**.  \n",
    "    In other words, columns or rows of $\\mathbf{Q}$ are orthonormal. \n",
    "\n",
    "- Note for a rectangular matrix $\\mathbf{Q} \\in \\mathbb{R}^{m \\times n}$, $m > n$, with orthonormal columns, we have $\\mathbf{Q}' \\mathbf{Q} = \\mathbf{I}_n$ but $\\mathbf{Q} \\mathbf{Q}' \\ne \\mathbf{I}_m$.\n",
    "\n",
    "- Any orthogonal matrix is full rank. And its inverse is its transpose.\n",
    "\n",
    "- Examples of orthogonal matrix: $\\mathbf{I}$, Householder matrix $\\mathbf{H} = \\mathbf{I} - 2 \\mathbf{u} \\mathbf{u}'$ with $\\|\\mathbf{u}\\|=1$, Jacobi matrix.    \n",
    "\n",
    "- More example: **Hadamard matrix**\n",
    "\\begin{eqnarray*}\n",
    "\\mathbf{H}_2 &=& \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}  \\\\\n",
    "\\mathbf{H}_4 &=& \\begin{pmatrix} \n",
    "1 & 1 & 1 & 1 \\\\ \n",
    "1 & -1 & 1 & -1 \\\\ \n",
    "1 & 1 & -1 & 1 \\\\\n",
    "1 & -1 & -1 & 1\n",
    "\\end{pmatrix} \\\\\n",
    "\\mathbf{H}_8 &=& \\begin{pmatrix} \\mathbf{H}_4 & \\mathbf{H}_4 \\\\ \\mathbf{H}_4 & -\\mathbf{H}_4 \\end{pmatrix}.\n",
    "\\end{eqnarray*}\n",
    "Are these orthogonal matrices? \n",
    "\n",
    "    **Hadamard conjecture** proposes that there is a $\\pm 1$ matrix with orthogonal columns whenever 4 divides $n$. [Wikipedia](https://en.wikipedia.org/wiki/Hadamard_matrix) says $n=668$ is the smallest of those sizes without a known Hadamard matrix.\n",
    "    \n",
    "- **Orthogonal matrix preserves length and angle.** If $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$ is an orthgonal matrix, then \n",
    "    1. $\\|\\mathbf{Q} \\mathbf{x}\\| = \\|\\mathbf{x}\\|$ for any $\\mathbf{x} \\in \\mathbb{R}^n$.  \n",
    "    2. $\\langle \\mathbf{Q} \\mathbf{x}, \\mathbf{Q} \\mathbf{y} \\rangle = \\langle \\mathbf{x}, \\mathbf{y} \\rangle$ for any $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$.\n",
    "    \n",
    "- If $\\mathbf{Q}_1, \\ldots, \\mathbf{Q}_k$ are othgononal matrices, then the product $\\mathbf{Q} = \\mathbf{Q}_1 \\cdots \\mathbf{Q}_k$ is orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal projections\n",
    "\n",
    "<img src=\"../06-matinv/three_projs.png\" width=600 align=\"center\"/>\n",
    "\n",
    "- If $\\mathcal{S}$ is a subspace of some vector space $\\mathcal{V}$ and $\\mathbf{y} \\in \\mathcal{V}$, then the projection of $\\mathbf{y}$ into $\\mathcal{S}$ along $\\mathcal{S}^\\perp$ is called the **orthogonal projection** of $\\mathbf{y}$ into $\\mathcal{S}$. \n",
    "\n",
    "- **The closest point theorem.** Let $\\mathcal{S}$ be a subspace of some vector space $\\mathcal{V}$ and $\\mathbf{y} \\in \\mathcal{V}$. The orthogonal projection of $\\mathbf{y}$ into $\\mathcal{S}$ is the **unique** point in $\\mathbf{S}$ that is closest to $\\mathbf{y}$. In other words, if $\\mathbf{u}$ is the orthogonal projection of $\\mathbf{y}$ into $\\mathcal{S}$, then\n",
    "$$\n",
    "    \\|\\mathbf{y} - \\mathbf{u}\\|^2 \\le \\|\\mathbf{y} - \\mathbf{w}\\|^2 \\text{ for all } \\mathbf{w} \\in \\mathcal{S},\n",
    "$$\n",
    "with equality holding only when $\\mathbf{w} = \\mathbf{u}$. \n",
    "\n",
    "    Proof: Picture. \n",
    "    \n",
    "- Let $\\mathbb{R}^n = \\mathcal{S} \\oplus \\mathcal{S}^\\perp$. A square matrix $\\mathbf{P}_{\\mathcal{S}}$ is called the **orthogonal porjector** into $\\mathcal{S}$ if, for every $\\mathbf{y} \\in \\mathbb{R}^n$, $\\mathbf{P}_{\\mathcal{S}} \\mathbf{y}$ is the projection of $\\mathbf{y}$ into $\\mathcal{S}$ along $\\mathcal{S}^\\perp$.  \n",
    "\n",
    "- For a matrix $\\mathbf{X}$, the orthogonal projector onto $\\mathcal{C}(\\mathbf{X})$ is written as $\\mathbf{P}_{\\mathbf{X}}$.  \n",
    "\n",
    "<img src=\"./ls_projection.png\" width=400 align=\"center\"/>\n",
    "\n",
    "- Let $\\mathbf{y} \\in \\mathbb{R}^n$ and $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$. \n",
    "    1. The orthogonal projector of $\\mathbf{y}$ into $\\mathcal{C}(\\mathbf{X})$ is given by $\\mathbf{u} = \\mathbf{X} \\boldsymbol{\\beta}$, where $\\mathbf{\\beta}$ satisfies the **normal equation**\n",
    "$$\n",
    "    \\mathbf{X}' \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}' \\mathbf{y}.\n",
    "$$\n",
    "Therefore,\n",
    "$$\n",
    "\\mathbf{P}_{\\mathbf{X}} \\mathbf{y} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^- \\mathbf{X}' \\mathbf{y}.\n",
    "$$\n",
    "for any generalized inverse $(\\mathbf{X}' \\mathbf{X})^-$.\n",
    "    2. If $\\mathbf{X}$ has full column rank, then the orthogonal projector into $\\mathcal{C}(\\mathbf{X})$ is given by\n",
    "$$\n",
    "    \\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'.\n",
    "$$\n",
    "\n",
    "    Proof of 1: Since the projection of $\\mathbf{y}$ into $\\mathcal{C}(\\mathbf{X})$ lives in $\\mathcal{C}(\\mathbf{X})$, thus can be written as $\\mathbf{u} = \\mathbf{X} \\boldsymbol{\\beta}$ for some $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$. Furthermore, $\\mathbf{v} = \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\in \\mathcal{C}(\\mathbf{X})^\\perp$ thus is orthogonal to any vectors in $\\mathcal{C}(\\mathbf{X})$ including the columns of $\\mathbf{X}$. Thus\n",
    "$$\n",
    "    \\mathbf{X}' (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}) = \\mathbf{0},\n",
    "$$\n",
    "or equivalently,\n",
    "$$\n",
    "    \\mathbf{X}' \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}' \\mathbf{y}.\n",
    "$$\n",
    "\n",
    "    Proof of 2: If $\\mathbf{X}$ has full column rank, $\\mathbf{X}' \\mathbf{X}$ is non-singular and the solution to the normal equation is uniquely determined by $\\boldsymbol{\\beta} = (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y}$, and the orthogonal projection is $\\mathbf{u} = \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{y}$. \n",
    "    \n",
    "- **Uniqueness of orthogonal projector.** Let $\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{n \\times p}$, both of full column rank and $\\mathcal{C}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{B})$. Then $\\mathbf{P}_{\\mathbf{A}} = \\mathbf{P}_{\\mathbf{B}}$. \n",
    "\n",
    "    Proof: Since $\\mathcal{C}(\\mathbf{A}) = \\mathcal{C}(\\mathbf{B})$, there exists a non-singular $\\mathbf{C} \\in \\mathbb{R}^{p \\times p}$ such that $\\mathbf{A} = \\mathbf{B} \\mathbf{C}$. Then\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{P}_{\\mathbf{A}} &=& \\mathbf{A} (\\mathbf{A}' \\mathbf{A})^{-1} \\mathbf{A}' \\\\\n",
    "    &=& \\mathbf{B} \\mathbf{C} (\\mathbf{C}' \\mathbf{B}' \\mathbf{B} \\mathbf{C})^{-1} \\mathbf{C}' \\mathbf{B}' \\\\\n",
    "    &=& \\mathbf{B} \\mathbf{C} \\mathbf{C}^{-1} (\\mathbf{B}' \\mathbf{B})^{-1} (\\mathbf{C}')^{-1} \\mathbf{C}' \\mathbf{B}' \\\\\n",
    "    &=&  \\mathbf{B} (\\mathbf{B}' \\mathbf{B})^{-1} \\mathbf{B}' \\\\\n",
    "    &=& \\mathbf{P}_{\\mathbf{B}}.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "- Let $\\mathbf{P}_\\mathbf{X}$ be the orthogonal projector into $\\mathcal{C}(\\mathbf{X})$, where $\\mathbf{X} \\in \\mathbb{R}^{n \\times p}$ has full column rank. Following statements are true.  \n",
    "    1. $\\mathbf{P}_\\mathbf{X}$ and $\\mathbf{I} - \\mathbf{P}_\\mathbf{X}$ are both symmetric and idemponent. \n",
    "    2. $\\mathbf{P}_\\mathbf{X} \\mathbf{X} = \\mathbf{X}$ and $(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{X} = \\mathbf{O}$.  \n",
    "    3. $\\mathcal{C}(\\mathbf{X}) = \\mathcal{C}(\\mathbf{P}_\\mathbf{X})$ and $\\text{rank}(\\mathbf{P}_\\mathbf{X}) = \\text{rank}(\\mathbf{X}) = p$.  \n",
    "    4. $\\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) = \\mathcal{N}(\\mathbf{X}') = \\mathcal{C}(\\mathbf{X})^\\perp$.  \n",
    "    5. $\\mathbf{I} - \\mathbf{P}_\\mathbf{X}$ is the orthogonal projector into $\\mathcal{N}(\\mathbf{X}')$ (or $\\mathcal{C}(\\mathbf{X})^\\perp)$.  \n",
    "    \n",
    "    Proof of 1: Check directly using $\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'$.  \n",
    "    \n",
    "    Proof of 2: Check directly using $\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'$. \n",
    "    \n",
    "    Proof of 3: Since $\\mathbf{P}_{\\mathbf{X}} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}'$, \n",
    "$$\n",
    "    \\mathcal{C}(\\mathbf{P}_\\mathbf{X}) \\subseteq \\mathcal{C}(\\mathbf{X}) = \\mathcal{C}(\\mathbf{P}_\\mathbf{X} \\mathbf{X}) \\subseteq \\mathcal{C}(\\mathbf{P}_\\mathbf{X}). \n",
    "$$\n",
    "\n",
    "    Proof of 4: The second equality is simply the fundamental theorem of linear algebra. For the first equality, first we show $\\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\subseteq \\mathcal{N}(\\mathbf{X}')$:\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{u} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} = (\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{v} \\text{ for some } \\mathbf{v} \\\\\n",
    "    &\\Rightarrow& \\mathbf{X}' \\mathbf{u} = [(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{X}]' \\mathbf{v} = \\mathbf{O} \\mathbf{v} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} \\in \\mathcal{N}(\\mathbf{X}').\n",
    "\\end{eqnarray*}\n",
    "To show the other direction $\\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\supseteq \\mathcal{N}(\\mathbf{X}')$,\n",
    "\\begin{eqnarray*}\n",
    "    & & \\mathbf{u} \\in \\mathcal{N}(\\mathbf{X}') \\\\\n",
    "    &\\Rightarrow& \\mathbf{X}' \\mathbf{u} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{P}_\\mathbf{X} \\mathbf{u} = \\mathbf{X} (\\mathbf{X}' \\mathbf{X})^{-1} \\mathbf{X}' \\mathbf{u} = \\mathbf{0} \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} = \\mathbf{u} - \\mathbf{P}_\\mathbf{X} \\mathbf{u} = (\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{u} \\\\\n",
    "    &\\Rightarrow& \\mathbf{u} \\in \\mathcal{C}(\\mathbf{I} - \\mathbf{P}_\\mathbf{X}).\n",
    "\\end{eqnarray*}    \n",
    "\n",
    "    Proof of 5: For any $\\mathbf{y} \\in \\mathbb{R}^n$, write $\\mathbf{y} = \\mathbf{P}_\\mathbf{X} \\mathbf{y} + (\\mathbf{I} - \\mathbf{P}_\\mathbf{X}) \\mathbf{y}$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "134.716px",
    "width": "251.989px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "583.693px",
    "left": "0px",
    "right": "1400.74px",
    "top": "110.284px",
    "width": "211.989px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
