{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network and Backpropagation (Chain Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History and recent surge\n",
    "\n",
    "From [Wang and Raj (2017)](https://arxiv.org/pdf/1702.07800.pdf):  \n",
    "<img src=\"./wangraj-table1.png\" width=400 align=\"center\"/>\n",
    "\n",
    "In a famous competition in 2012, the AlexNet (60 million weights) classified 1.2 million images in ImageNet with an error rate half of the next best one. This brings the current wave of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning sources\n",
    "\n",
    "- [_Linear Algebra and Learning from Data_](https://math.mit.edu/~gs/learningfromdata/) by Gil Strang.\n",
    "\n",
    "- _Elements of Statistical Learning_ (ESL) Chapter 11: <https://web.stanford.edu/~hastie/ElemStatLearn/>.\n",
    "\n",
    "- UFLDL: <http://ufldl.stanford.edu/tutorial/>.\n",
    "\n",
    "- Stanford CS231n: <http://cs231n.github.io>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer neural network (SLP)\n",
    "\n",
    "- Aka single layer perceptron (SLP) or single hidden layer back-propagation network.\n",
    "\n",
    "- Sum of nonlinear functions of linear combinations of the inputs, typically represented by a **network diagram**.  \n",
    "<img src=\"./esl-fig-11-2.png\" width=400 align=\"center\"/>\n",
    "\n",
    "    TODO: redo above diagram to match notations.\n",
    "\n",
    "<!---\n",
    "- Mathematical model:\n",
    "\\begin{eqnarray*}\n",
    "Z_m &=& \\sigma(\\alpha_{0m} + \\alpha_m^T X), \\quad m = 1, \\ldots, M \\\\\n",
    "T_k &=& \\beta_{0k} + \\beta_k^T Z, \\quad k = 1,\\ldots, K \\\\\n",
    "Y_k &=& f_k(X) = g_k(T), \\quad k = 1, \\ldots, K.\n",
    "\\end{eqnarray*}\n",
    "\n",
    "    - **Output layer**: $Y=(Y_1, \\ldots, Y_K)$ are $K$-dimensional output.         For univariate response, $K=1$; for $K$-class classification, $k$-th unit models the probability of class $k$.  \n",
    "    \n",
    "    - **Input layer**: $X=(X_1, \\ldots, X_p)$ are $p$-dimensional input features. \n",
    "    \n",
    "    - **Hidden layer**: $Z=(Z_1, \\ldots, Z_M)$ are derived features created from linear combinations of inputs $X$.\n",
    "    \n",
    "    - $T=(T_1, \\ldots, T_K)$ are the output features that are directly associated with the outputs $Y$ through output functions $g_k(\\cdot)$.\n",
    "    \n",
    "    - $g_k(T) = T$ for regression. $g_k(T) = e^{T_k} / \\sum_{k=1}^K e^{T_k}$ for $K$-class classification (**softmax regression**).\n",
    "    \n",
    "    - Number of **weights** (parameters) is $M(p+1) + K(M+1)$.\n",
    "-->\n",
    "\n",
    "- Mathematical model:\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{A}_2[(\\mathbf{A}_1 \\mathbf{v}_0 + \\mathbf{b}_0)_+].\n",
    "$$\n",
    "\n",
    "    - **Input layer**: $\\mathbf{v}_0=(v_{01}, \\ldots, v_{0p})$ are $p$-dimensional input features. \n",
    "\n",
    "    - **Hidden layer**: $\\mathbf{v}_1 = \\mathbf{A}_1 \\mathbf{v}_0 + \\mathbf{b}_1$ are derived features created from linear combinations of inputs $\\mathbf{v}$.\n",
    "\n",
    "    - **Activation function**: A nonlinear activation function, e.g., $\\text{ReLU}(x) = x_+ = \\max(x, 0)$, is applied componentwise to the derived features to produce $(\\mathbf{v}_1)_+ = (\\mathbf{A}_1 \\mathbf{v}_0 + \\mathbf{b}_1)_+$.\n",
    "    \n",
    "    - **Output layer**: A function maps the derived features to the $K$-dimensional output. For example,\n",
    "$$\n",
    "\\mathbf{v}_1 \\mapsto \\mathbf{A}_2 \\mathbf{v}_1 + \\mathbf{b}_2,\n",
    "$$ \n",
    "where $\\mathbf{A}_2 \\in \\mathbb{R}^{K \\times q}$ and $\\mathbf{b}_2 \\in \\mathbb{R}^K$. \n",
    "    \n",
    "    - **Loss function**: A loss function $L(\\mathbf{w}, \\mathbf{y})$ that measures the fitness of the output $\\mathbf{w}$ to training labels.\n",
    "        \n",
    "    - Number of **weights** (parameters) is $q(p+1) + K(q+1)$.\n",
    "\n",
    "- **Activation function** $\\sigma$:  \n",
    "\n",
    "    - $\\sigma(v)=$ a step function: human brain models where each unit represents a neuron, and the connections represent synapses; the neurons fired when the total signal passed to that unit exceeded a certain threshold.\n",
    "\n",
    "    - **Sigmoid** function:  \n",
    "$$\n",
    "\\sigma(v) = \\frac{1}{1 + e^{-v}}.\n",
    "$$\n",
    "<img src=\"./esl-fig-11-3.png\" width=400 align=\"center\"/>\n",
    "    \n",
    "    - **Rectifier**. $\\sigma(v) = v_+ = max(0, v)$. A unit employing the rectifier is called a **rectified linear unit (ReLU)**. According to Wikipedia: _The rectifier is, as of 2018, the most popular activation function for deep neural networks_.    \n",
    "\n",
    "    - **Softplus**. $\\sigma(v) = \\log (1 + \\exp v)$.  \n",
    "<img src=\"./Rectifier_and_softplus_functions.svg\" width=400 align=\"center\"/>    \n",
    "- **Loss function**: Given training data $(\\mathbf{v}_{0,1}, y_1), \\ldots, (\\mathbf{v}_{0,n}, y_n)$, the loss function $L$ can be:\n",
    "\n",
    "    - Sum of squared error (SSE):\n",
    "$$\n",
    "L = \\frac 12 \\sum_{k=1}^K \\sum_{i=1}^n [y_{ik} - w_{ik}]^2.\n",
    "$$\n",
    "    \n",
    "    - Cross-entropy (deviance):\n",
    "$$\n",
    "L = - \\sum_{k=1}^K \\sum_{i=1}^n y_{ik} \\log p_{ik}.\n",
    "$$\n",
    "For $K$-class classification, $k$-th unit models the probability of class $k$. The softmax function\n",
    "$$\n",
    "p_{ik} = \\frac{e^{w_k}}{\\sum_k e^{w_k}}, \\quad k = 1,\\ldots,K.\n",
    "$$\n",
    "    \n",
    "- Model fitting: **back-propagation** (gradient descent) to find the weights $\\mathbf{A}_i, \\mathbf{b}_i$ that minimize the loss function.   \n",
    "    - For notational simplicity, let's ignore the bias/intercept term $\\mathbf{b}_1$.  \n",
    "    - Then the network structure is\n",
    "$$\n",
    "\\mathbf{v}_0 \\mapsto \\mathbf{v}_1 = \\mathbf{A}_1 \\mathbf{v}_0 \\mapsto \\mathbf{w} = \\mathbf{A}_2 \\sigma(\\mathbf{v}_1) \\mapsto L(\\mathbf{w}, \\mathbf{y})\n",
    "$$\n",
    "or\n",
    "$$\n",
    "L(\\mathbf{v}_0, \\mathbf{y}) = L(\\mathbf{A}_2\\sigma(\\mathbf{A}_1\\mathbf{v}_0), \\mathbf{y}).\n",
    "$$\n",
    "\n",
    "    - Consider the sum of squared error with $K=1$\n",
    "\\begin{eqnarray*}\n",
    "L = \\sum_i L_i, \\text{ where } L_i &=& (y_i - w_i)^2.\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "    - The derivatives for the $i$-th training data point:\n",
    "\\begin{eqnarray*}\n",
    "    \\frac{\\partial L_i}{\\partial \\mathbf{A}_2} &=& \\frac{\\partial L_i}{\\partial w_i} \\cdot \\frac{\\partial w_i}{\\partial \\mathbf{A}_2} = - (y_{i} - w_i) \\sigma(\\mathbf{v}_{1,i}) \\\\\n",
    "    \\frac{\\partial L_i}{\\partial \\mathbf{A}_1} &=& \\frac{\\partial L_i}{\\partial w_i} \\cdot \\frac{\\partial w_i}{\\partial \\mathbf{v}_{1,i}} \\cdot \\frac{\\partial \\mathbf{v}_{1,i}}{\\partial \\mathbf{A}_1} = - (y_{i} - w_i) \\text{diag}(\\sigma'(\\mathbf{v}_{1,i})) \\mathbf{A}_2' \\mathbf{v}_{0,i}'.\n",
    "\\end{eqnarray*}  \n",
    "    Above is _bad_ notation. Next lecture we learn how to derive chain rule properly.  \n",
    "    - Gradient descent update:\n",
    "\\begin{eqnarray*}\n",
    "    \\mathbf{A}_{2}^{(t+1)} &=& \\mathbf{A}_{2}^{(t)} - s^{(t)} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial \\mathbf{A}_{2}^{(t)}} \\\\\n",
    "    \\mathbf{A}_{1}^{(t+1)} &=& \\mathbf{A}_{1}^{(t)} - s^{(t)} \\sum_{i=1}^n \\frac{\\partial L_i}{\\partial \\mathbf{A}_{1}^{(t)}},\n",
    "\\end{eqnarray*}\n",
    "where $s^{(t)}$ is the **learning rate**.\n",
    "    \n",
    "    - Two-pass updates: for each training data point,\n",
    "\\begin{eqnarray*}\n",
    "    & & \\text{current } \\mathbf{A}_1, \\mathbf{A}_2 \\to \\mathbf{v}_{1,i} \\to \\mathbf{w}_i \\to \\text{ evalulate loss } L_i \\quad \\quad \\quad \\text{(forward pass)}   \\\\\n",
    "    &\\to& \\text{evaluate } \\frac{\\partial L_i}{\\partial \\mathbf{A}_2} \\to \\text{evaluate } \\frac{\\partial L_i}{\\partial \\mathbf{A}_1} \\to \\text{ update } \\mathbf{A}_2 \\text{ and } \\mathbf{A}_1 \\quad \\quad \\text{(backward pass)}.\n",
    "\\end{eqnarray*}\n",
    "    \n",
    "    - Advantages: each hidden unit passes and receives information only to and from units that share a connection; can be implemented efficiently on a parallel architecture computer.\n",
    "    \n",
    "<!-- - Alternative fitting methods: conjugate gradients, variable metric methods. -->\n",
    "\n",
    "- Stochastic gradient descent (**SGD**). In real machine learning applications, training set can be large. Backpropagation over all training cases can be expensive. Learning can also be carried out **online** â€” processing each batch one at a time, updating the gradient after each training batch, and cycling through the training cases many times. A training **epoch** refers to one sweep through the entire training set.\n",
    "\n",
    "    **AdaGrad**, **RMSProp**, and **ADAM** improve the stability of SGD by trying to incorpoate Hessian information in a computationally cheap way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressivity of neural network\n",
    "\n",
    "- Playground: <http://playground.tensorflow.org>\n",
    "\n",
    "- Sources:  \n",
    "    - [On the expressive power of deep neural network](https://arxiv.org/abs/1606.05336).  \n",
    "    - [On the number of response regions of deep feed forward networks with piece-wise linear activations](https://arxiv.org/abs/1312.6098).  \n",
    "\n",
    "- Consider the function $F: \\mathbb{R}^m \\mapsto \\mathbb{R}^n$\n",
    "$$\n",
    "F(\\mathbf{v}) = \\text{ReLU}(\\mathbf{A} \\mathbf{v} + \\mathbf{b}).\n",
    "$$\n",
    "Each equation\n",
    "$$\n",
    "\\mathbf{a}_i \\mathbf{v} + b_i = 0\n",
    "$$\n",
    "creates a hyperplane in $\\mathbb{R}^m$. ReLU creates a _fold_ along that hyperplane. There are a total of $n$ folds.  \n",
    "    - When there are $n=2$ hyperplanes in $\\mathbb{R}^2$, 2 folds create 4 pieces.  \n",
    "    - When there are $n=3$ hyperplanes in $\\mathbb{R}^2$, 3 folds create 7 pieces. \n",
    "    \n",
    "- The number of linear pieces of $F$ and regions bounded by the $N$ hyperplanes is\n",
    "$$\n",
    "r(n, m) = \\sum_{i=0}^m \\binom{n}{i} = \\binom{n}{0} + \\cdots + \\binom{n}{m}.\n",
    "$$\n",
    "\n",
    "    Proof: Induction using the recursion\n",
    "$$\n",
    "r(n, m) = r(n-1, m) + r(n-1, m-1).\n",
    "$$\n",
    "\n",
    "- Corollary:  \n",
    "    - When there are relatively few neurons $n \\ll m$, \n",
    "$$\n",
    "r(n,m) \\approx 2^n.\n",
    "$$\n",
    "    - When there are many neurons $n \\gg m$, \n",
    "$$\n",
    "r(n,m) \\approx \\frac{n^m}{m!}.\n",
    "$$\n",
    "\n",
    "- Counting number of flat pieces with more hidden layers is much harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Multi-layer neural network (MLP) = function composition\n",
    "\n",
    "- Aka multi-layer perceptron (MLP).\n",
    "\n",
    "- 1 hidden layer:  \n",
    "<img src=\"./ufldl-network-331.png\" width=400 align=\"center\"/>   \n",
    "\n",
    "- 2 hidden layers:  \n",
    "<img src=\"./ufldl-network-3322.png\" width=400 align=\"center\"/>  \n",
    "\n",
    "- In each layer,\n",
    "$$\n",
    "\\mathbf{v}_k = F_k(\\mathbf{v}_{k-1}) = \\text{ReLU}(\\mathbf{A}_k \\mathbf{v}_{k-1} + \\mathbf{b}_k)\n",
    "$$\n",
    "and the overall MLP with $L$ layers corresponds to function composition\n",
    "$$\n",
    "\\mathbf{w} = F_L(\\ldots F_2(F_1(\\mathbf{v}_0))).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation properties\n",
    "\n",
    "- Boolean Approximation: an MLP of one hidden layer can represent any boolean function exactly.\n",
    "\n",
    "- Continuous Approximation: an MLP of one hidden layer can approximate any bounded continuous function with arbitrary accuracy.\n",
    "\n",
    "- Arbitrary Approximation: an MLP of two hidden layers can approximate any function with arbitrary accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Practical issues\n",
    "\n",
    "Neural networks are **not a fully automatic tool**, as they are sometimes advertised; as with all statistical models, subject matter knowledge should and often be used to improve their performance.\n",
    "\n",
    "- Starting values: usually starting values for weights are chosen to be random values near zero; hence the model starts out nearly linear (for sigmoid), and becomes nonlinear as the weights increase.\n",
    "    \n",
    "- Overfitting (too many parameters):\n",
    "    1. early stopping; \n",
    "    2. weight decay by $L_2$ penalty  \n",
    "$$\n",
    "L(\\mathbf{A}_1, \\mathbf{A}_2) + \\frac{\\lambda}{2} \\left( \\|\\mathbf{A}_1\\|_{\\text{F}}^2 + \\|\\mathbf{A}_2\\|_{\\text{F}}^2 \\right),\n",
    "$$\n",
    "where $\\lambda$ is the **weight decay parameter**.  \n",
    "    3. **Dropout**. At each training case, individual nodes are either dropped out of the net with probability $1-p$ or kept with probability $p$, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Forward and backpropagation for that training case are done only on this thinned network.   \n",
    "<img src=\"./dropout.png\" width=400 align=\"center\"/>  \n",
    "Figure from [Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov (2014)](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf).\n",
    "\n",
    "- Scaling of inputs: mean 0 and standard deviation 1. With standardized inputs, it is typical to take random uniform weights over the range [âˆ’0.7,+0.7].\n",
    "    \n",
    "- How many hidden units and how many hidden layers: guided by domain knowledge and experimentation.\n",
    "    \n",
    "- Multiple minima: try with different starting values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution neural network (CNN)\n",
    "\n",
    "- [CNN](http://hua-zhou.github.io/teaching/biostatm280-2019winter/slides/15-nn/nn1.html#convolutional-neural-networks-cnn) is a special structure in matrix $\\mathbf{A}$ that utilizes the local stationarity of natural images.\n",
    "\n",
    "- Stride, pooling, etc.\n",
    "\n",
    "- Example: [AlexNet](http://hua-zhou.github.io/teaching/biostatm280-2019winter/slides/15-nn/nn1.html#example-image-classification) for image classification.\n",
    "\n",
    "- The structure of AlphaGo Zero:  \n",
    "    1. A convolution of 256 filters of kernel size $3 \\times 3$ with stride 1.   \n",
    "    2. Batch normalization.   \n",
    "    3. ReLU.  \n",
    "    4. A convolution of 256 filters of kernel size $3 \\times 3$ with stride 1.  \n",
    "    5. Batch normalization.   \n",
    "    6. A skip conneciton as in ResNets that adds the input to the block.  \n",
    "    7. ReLU.  \n",
    "    8. A fully connected linear layer to a hidden layer of size 256.  \n",
    "    9. ReLU.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
